# ğŸš† IRCTC Real-Time Data Pipeline (GCP)  

A real-time streaming pipeline that captures IRCTC ticket booking data using **Pub/Sub**, processes it with **Dataflow** via a **UDF**, stores raw data in **GCS**, and writes structured results to **BigQuery**  

---

## ğŸ“Œ Architecture & Flow  

```
[ Pub/Sub Topic ]
        â†“
[ Dataflow Pipeline ] â†’ UDF transforms â†’ Read function from GCS and also store
the rejected data(Google Cloud Storage) â†’ Writes to BigQuery (analytics)
```

### ğŸ”„ Steps:
1. **Simulate or stream IRCTC booking events** â†’ Pub/Sub  
2. **Dataflow ingestion** â†’ Reads from Pub/Sub via subscription  
3. **Apply UDF transform** â†’ Clean/enrich data  
4. **read udf from** â†’ GCS bucket  
5. **Load structured data** â†’ Write to BigQuery tables  

---

## ğŸ› ï¸ Tech Stack & Components  

- ğŸš€ **Google Cloud Platform (GCP)**
- ğŸ“© **Pub/Sub** â€“ Streaming ingestion of ticket events  
- âš™ï¸ **Dataflow** â€“ Managed Apache Beam pipeline with UDF  
- ğŸ—„ï¸ **Google Cloud Storage (GCS)** â€“ Raw data backup and storage  
- ğŸ“Š **BigQuery** â€“ Queryable analytics datastore  

---

## ğŸš€ Getting Started  

### âœ… Prerequisites
- GCP account with billing enabled  
- Permissions: Pub/Sub, Dataflow, GCS, BigQuery  
- Local setup:  
  ```bash
  gcloud auth login  
  gcloud config set project YOUR_PROJECT_ID  
  ```

### 1ï¸âƒ£ Provision Infrastructure

```bash
# Create Pub/Sub resources
gcloud pubsub topics create irctc-topic
gcloud pubsub subscriptions create irctc-sub --topic=irctc-topic

# GCS for raw backups
gsutil mb -l REGION gs://YOUR_BUCKET_NAME

# BigQuery dataset/table
bq mk YOUR_DATASET
# Optional: define schema for table
```

### 2ï¸âƒ£ Pipeline Code Setup

```bash
git clone https://github.com/Ravindrak0812/irctc_real_time_data_pipeline_data_engineering_project.git
cd irctc_real_time_data_pipeline_data_engineering_project
```

Key files:  
- `pipeline.py` â€“ Dataflow pipeline logic  
- `udf.py` â€“ Custom UDF transformations  
- `data_generator.py` â€“ Event generator for IRCTC bookings  

### 3ï¸âƒ£ Run Locally (DirectRunner)

```bash
python data_generator.py --topic irctc-topic
python pipeline.py   --input_topic projects/${PROJECT_ID}/topics/irctc-topic   --output_gcs gs://YOUR_BUCKET_NAME/raw/   --output_bq ${PROJECT_ID}:YOUR_DATASET.your_table   --runner DirectRunner
```

### 4ï¸âƒ£ Deploy to Dataflow (Production)

```bash
python pipeline.py   --input_topic projects/${PROJECT_ID}/topics/irctc-topic   --output_gcs gs://YOUR_BUCKET_NAME/raw/   --output_bq ${PROJECT_ID}:YOUR_DATASET.your_table   --runner DataflowRunner   --project ${PROJECT_ID}   --region YOUR_REGION   --temp_location gs://YOUR_BUCKET_NAME/temp/   --job_name irctc-dataflow-job
```

### 5ï¸âƒ£ Verify Data

- GCS raw files â†’ `gsutil ls gs://YOUR_BUCKET_NAME/raw/`  
- BigQuery â†’ Query transformed structured data  

---

## ğŸŒŸ Optional Enhancements  

- Schema evolution automation in BigQuery  
- Enrichment in UDF (geo, metadata, validation)  
- Monitoring with Stackdriver & alerts  
- Visualization dashboards in Looker Studio  

---

## ğŸ“ Tips & Best Practices   

- Regularly **clean up GCS and BigQuery** to manage costs  
- Batch small files in GCS to prevent bloat  
- Use **Dataflow templates** for recurring jobs  
- Add retries and error-handling in UDF for resilience  

---


